12.7-Deep learning-ImageNet y redes convolucionales
https://www.youtube.com/watch?v=nUHi0J97BHk
 e imágenes en la casete que hizo posible todo este resurgimiento de clarín y de mantienen en general el zonda casi a 15 años en crearse de investigadores principalmente estamos poniendo universidades nuestros sonidos trabajaron por ahí te quitando a mano una foto y que lo que se veía en la foto hasta que lograron de la nación de por lo menos un millón de de un millón de imágenes de grababa y esas imágenes son las suficientes para poder empezar a entrenar y si logran estos resultados los resultados y la clave es una competencia o fue una competencia tienen la competencia en diputado imágenes que se desarrollaron varios años por ejemplo una muestra en aquí por ejemplo para el año 2010 el equipo ganador obtuvo un 28% con técnicas tradicionales de escritores locales y cottbus y más fácil el año 2011 mejor y las técnicas cada vez más complicadas la mejora de un 2 por ciento del ganador en el error tus cinco de aquí nuestra hermana más bajo mejor el año 2000 y entonces el mejor equipo hasta un 28 por ciento del round top 5 y luego el otro año con técnicas cada vez más a la hora un 26 y diego del año 2012 y sacó con una red convencional la llamada lex net saco un 16% hay que notar esa diferencia de la diferencia este 10% de diferencia fue tan demoledor que todo está basado técnica antigua y esta fue la primera vez que el equipo de donde estaba [Música] esa diferencia fue tan demoledora que todos los investigadores todos en el área del grupo de la comunidad de investigación del viernes y toda la investigación que se estaba realizando con de escritores locales y code books y toda la del avance que se está haciendo ahí todo quedó botado y todos del año siguiente al año 2013 todo toda la comunidad científica desarrollando redes convolución ales replicando el trabajo de la lex net para tratar de entender por qué obtenía esos resultados es decir nadie con las técnicas anteriores nadie podía siquiera tener esperanza me llegaron 16 por ciento de entonces este resultado fue pero el demoledor era curioso en esa época en la cual explica todos tratando de entender quién tiene dentro de las redes convencionales [Música] y de hecho y el año siguiente estas cepas y fin es curioso pero visto bueno y no hizo ninguna muerte con respecto a la lex net en más técnica sino que lo que hizo fue explicar por qué ha funcionado a la lex net y ajustar la mejor ajusta a lo mejor nos daba a controlar lo mejor es poner la misma arquitectura sólo que mejor entrado y porque están bien entendiendo por qué estaba funcionando y en el paper tratan de explicarlo de forma más clara a toda la comunidad científica porque funciona la red como nacional y porque logran necesitarlo hasta el 2013 nuestro del 2014 en adelante en pesos ya estaban todos los tantos investigación y universidades e investigando en redes convulsionar y probando en distintas arquitecturas y ahí es donde empezaron a aparecer muchas aventuras distintas a partir de 2014 y ahí aparece la google por ejemplo que fue conocido coste alto con casi un 75 por ciento y después de ello siguiente a presión ascendente en el año 2015 que fue aquí y aquí decir tu distrito transantiago santiago a chile en la excepción aquí se anunció la res net y que lograba un 4% y este resultado ya era significativo porque aparte de que el resultado que lograba era ya en lo que se conocía como quiere el resultado del humano de las personas para clasificar en este problema así que ya se ve igual como igual de bueno que las personas así que el problema se razón y lo otro interesante la razón era que así como eran ocho capas después 22 capas y allí otras dos de arquitectura que nos montando la capa y llegaron a 152 capas y de 152 y no más y era porque 'son al máximo que pudieron con sus servidores de 87 con con máxima memoria era la magia obtuviera aceleran 152 capas y eso fue lo que hicieron y ganaron con eso así que el 10 otro resulta mente es el que explica a la comunidad científica que la profundidad era muy importante ahora como la hacen cómo hacer esa profundidad interesante porque no es la misma lexnet con más camas porque eso no no eso no convergen sino una nueva forma para terminales luego del 2015 es como si el problema en desuso o el poco interés porque ya el problema está resuelto y en realidad siempre sano moverlo a comer científica de otros programas detectar por detectar varios objetos en la misma imagen detectar la zona segmentar imágenes clasificación y un error 2 un breve resumen de la historia así como el alex nerd apareció en 2012 la reconversión del 98 [Música] no era para esta imagen existían pero estar enfocadas en resolver un problema que era el de reconocer dígitos manuscritos del 0 al 9 y después en el números eran muy buenos hay eran imbatibles pero nadie no eran muy famosas ni vinieron muy interesantes porque no resolvían un problema muy interesante porque el hueso porque cuando resuelve el problema de imágenes ahí si éstas hacen famosa porque finalmente dar una muestra de que esta red conclusión a les pueden resolver un problema que nadie más área en este por ejemplo en este problema de detectar dígitos y la reconversión a les ganaban pero grande los polis sacan algo así como 99 97 por ciento de precisión versus las otras técnicas que sacaban como un 98,5 cosas entonces ganaban pero el problema no no no tenía mucho interés porque era como que ya estaba resuelto que las técnicas clases de entonces la revolución a les ganaban pero necesitar un problema más difícil para poder demostrar entonces realmente cuando lo logran porque en este año en esta época hénin factible de clasificar en imágenes naturales fotografías porque el lexnet del 2012 no fue sólo la arquitectura la interesante sino que como lo hicieron era porque aquí no existía cuda sino que estaba recién partiendo y todo el que programa existe al gpu pero a través de los filtros de shaders y que y que tiene que programar las convulsiones mediante un lenguaje que no era para la computación en general y además aquí en este original está dividido en dos porque a mano quien dividirla para usar dos tipos diferentes entonces fue un trabajo de desarrollo de ingeniería muy grande para poder procesar ese millón de imágenes y con esta red convencional esta regla es muy importante porque fue la que mostró al mundo y uruguay dio el interés por la red convencional y con la red es ahora ligera en su arquitectura y lo que interesa mostrar es que son cinco conclusiones pero tiene un tamaño 227 227 es decir la imagen no es muy grande tiene 5 convolución es de 96 fiestas 128 finitos interactivos filtros 58 filtros y luego de esos que son las tres placas y luego de eso viene dos capas full y conecte entonces está la convulsión 5 luego estaban las variantes 6 2 phillip island conecte 7 y luego la capa final que es la de problemas de 2026 no nos den y eso y que la previa la previa a las 1216 con seis y por tanto el entre en esta zona hay una capa azul y conecta entre 9.200 y 4.096 ya la cantidad de millones parámetros que ella y él hace que sean 37 millones de parámetros solo entre medias dos camas y hace que él y él todo el peso de la regla en total tiene 60 millones de parámetros y más de la mitad porque la espiritualidad para producir tamaño que aquí en esta parte para que para quitar peso aquí es el dibujo el pp después aquí ellos dan una imagen un poco más bonita finalmente lo que interesa es ver como una imagen se va reduciendo por medio del bullying y se va haciendo más profundo porque va teniendo a cada vez más filtros hasta que llega a este tamaño que es de 6 x 6 x 12 56 56 y eso celular se platean y aparecen las capas olivares y que el descriptor visual que se puede sacar en realidad se están demostrando capaces él tiene cinco convulsiones el paper este del año 2013 explican intentan entender qué es lo que está entrenando los filtros para entender cómo funcionan las convencionales la muestra que muestra son filtros simples tipo sobre detección de colores los vistos de capa dos personas que están uniendo estos filtros de capa uno se ve que están detectando y asegura y un poco más complejas figuras geométricas y los filtros capaces que un grafito acabado ya pueden detectar objetos de los patrones de texturas como se ven como se viene en esta imagen atacar los de capa 4 que unen estas texturas ya pueden detectar figuras figuras complejas se ve ya sabe que puede inyectar mi cara de plenos bueno y lo de la última cábala capaz de detectar a los autores entonces el mismo 5 capas ya detecta el detecta dos figuras visuales son los patrones que hay que intenta figuras complejas eso eso fuera parte del año 2003 tras 2014 la google net empezó a elaborar distintas arquitecturas y en el que el tamaño de la fiesta será también 357 él era una decisión de nivel parámetro lo que dijeron aquí es que inquieta si es que probamos todos hagamos convolución es de 3 5 y 7 y con cat éramos todo resultado que entrenen ahí verá cuál es la convulsión que mal de cierre y está estos módulos de probar todas las convulsiones los tamaños de convulsión y después juntarlos para ver si es cualquiera mejor se le llama en bloque de inserción la red google que es una liviana porque tiene un campo público un activo y que fue el mejor resultado de ese año ese año también estuve en las vírgenes del sol donde es muy la red de gg es muy parecida a la lex net es la misma arquitectura las 5 convención normal las dos camas full y con defectos que ahora las hace un poco más pesadas porque era una cima convencional a subdivide en más convulsiones todavía pero les siguen llamando a composición 1 2 4 5 pero le llaman convolución capa convolución de acabado conclusión 1 como son dos tratan de mantener la nomenclatura de los nombres para llamarle finalmente la capacidad la capa se sigue llamando las 1617 ahí es donde tenemos el descriptor algo que refleje bien bueno lo que tiene en contra es que si la lexnet de las grandes o 60 millones de pararrayos en la vejez es aún más grande y tiene el peso como 500 megas es como cinco veces más grande que en roma se siente mucho es más grande y la lex net en la rejilla y logra resultados mejores pero lo mejor es que unas líneas para otros un dato interesante la vejez es que primero que entrenar esta misma arquitectura con datos de rostros y hay una red de jefes quien permite detectar y reconocer escalas y calcular similitud entre caras y cuales sólo puede ser autores de rostros de personas del 2000 se presenta una red de 152 capas ya que fue la que obtuvo el mejor resultado este año bueno aquí en realidad está haciendo 51 34 pero lo interesante es que no son capas como las de la lex net que son como un oso como lo son en conclusión sino que las colusiones tienen lo que se llama sterger y que eventualmente el dato puede pasar por las convulsiones o se une con el dato 55 involución cortado original a eso se le llama el bloque nacido entonces el dato original se lanza con solución pero aquí el dato original también traspasa sin modificación y se con cadenas hace una se suman 60 se combinan porque quiero hacer esto y aquí está el truco porque es para el bar lo que ha hecho y el val profesión va a modificar estos pesos pero sí que fue y además el país esto puede pasar si como en la identidad pueden pasar sin sin modificación a la capa anterior por tanto si se va yendo por fuera puede tener oportunidad de a partir de la última salida y un diferencial añada por ver el diferencial es lograr modificar la primera capa de hesse quien no tiene este bloque residual pisasen convolución de convolución estos diferenciales diferenciales diferenciales finalmente no logró modificar la capa desaparece y por tanto es la la la mejora o el truco que presenta [Música] quien lo que permite entrenar una cama de esta profundidad notar que cuando veamos donde hay un peso compartido y los pasos que abordan los siguientes etapas de los tiempos de la recolecta van operando sobre un paso son un paso compartido [Música] y la última recta como destacable desde esta serie en el examen la suscripción existente en el 2017 y que esencialmente lo que hace es que como las convenciones apenas sobre todos las toneladas sobre toros para el mismo tiempo que ellos decían que qué tal si le damos la oportunidad de la red de favorecer 111 canales por eso nosotros y que las comunidades no sean tan igualitarias de todos los canales sino que vamos a entregar un filtro quien pondera los canales y de esta forma en las convenciones posteriores con un peso con los canales ya escala 2 al canal más importante o menos importante y muestran ahí que los resultados mejor mejoran y que no requieren más parámetros y de hecho lo son redes cada vez la investigación de profesión ha enfocado en hacer además lee bien y que los mismos resultados que la red la red está empezar ya como la resina de la vejez