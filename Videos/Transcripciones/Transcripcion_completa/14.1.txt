14.1-Buscador de imágenes sin etiquetar en python
https://www.youtube.com/watch?v=Rr0eRlGDKiI
 hola en este vídeo vamos a revisar un ejemplo de cómo implementar un buscador por texto en imágenes que están etiquetadas y cómo hacemos eso en la clave es usar un conjunto de entrenamiento donde tengamos muchas imágenes que están etiquetadas y vamos a entrenar la relación entre el escritor de texto y de escritor ni imagen y por tanto vamos a poder saber la relación entre palabras y lo que se ve en la foto de entrenamos eso y luego lo aplicamos sobre un conjunto de esta etiqueta entonces para poder entrenar en nuestra relación palabra con imagen en ciudad mucha paciencia imagen y etiquetas y vamos al ejemplo del coco data set y esta es una competencia existen risas de varios años y tienen muchos datos publicados y no etiquetados se le conoce como el ms coco porque inicialmente lo publicó microsoft pero ahora ya hay una agrupación que contiene más de sus pasos en particular si vamos a los datos casetes podemos ver por ejemplo aquí vamos a ver para hacer de 2014 las imágenes que están aquí se pueden descargar vamos a las de 32 gigas y además están las descripciones de esto se puede explorar y podemos ver marquis está explorando ha sido tomamos constante al 2017 vamos a buscar fotos cualquiera para mostrar como cómo está esto d en este data se realiza esto como para segmentación y para búsqueda de objetos a nosotros nos va a interesar en la descripción de cada foto tiene está descrita por de cinco formales como cinco personas describieron cada foto entonces aquí vemos sus descripciones palabras y lo que sabíamos que el descriptor de imágenes de esta fuerza por ejemplo estas descripciones están en inglés y lo que es y lo que vamos a esta descripción es de una de las descargue y en las tradujo al español usando el descriptor abierto de google hay otras formas más jirafas que está ahí y hay más la escena aquí entonces como hay 5 descripción de texto entonces lo que vamos hacer primero es que eso es un conjunto y luego lo prepare para descargarlo aquí junto con este ejemplo por la velocidad esto ya está ya de este código aquí está el descargo los datos de objetos data donde aquí tiene el campeón tiene tenemos 20 mil imágenes cada una con 5 captions lo que nos da entonces tenemos 100 mil captions mil textos y 20 mil imágenes de entrenamiento y con eso con esos datos de entrenamiento de el cual canta en español vamos al primero tengo que extraer en los descriptores visuales de aquí podemos usar él el defensor que vimos en el ejemplo anterior de donde donde mostramos cómo calcular descriptores usando que era si aquí hay una vez 152 pekín de esta función ya la revisamos dientes y aquí está como lo calcula y lo guarda para calcular el calculado y aquí hay una pequeña revisión de los datos de entrenamiento entonces de las 20.000 imágenes aquí está imprimiendo la imagen y la descripción y las cinco descripción los cinco caption que tiene están en español en español con traducción automática así que igual hay algunas descripciones que se lee en un extremo [Música] y tenemos cinco conjuntos de entrenamiento de mil imágenes que no los vamos a tener solamente para evaluar pero aquí podemos buscar otras imágenes que producen imágenes [Música] la imagen 20 es esta pista y la imagen 330 lo que vamos a hacer entonces es que primero los vectores tenemos que calcular su vez un diagrama lo que vamos a hacer tenemos muchos textos por ejemplo no es decir en una jirafa sin afán corriendo y tenemos la foto de la jirafa [Música] el de aquí va a decir a las cebras cebra jugando y tenemos en la foto de en el láser de aquí y para cada una de estas estas imágenes vamos a calcular su vector de respeto' importante ese vector describe en la parte visual de x lo que se ve en la imagen entonces este vector de aquí para que es como que tiene la forma visual de una jirafa y aquí tiene la forma de una el escritor y del texto vamos vamos a generar su vector de texto que en principio vamos a steffi de f podemos usar incluso por tve cosas más matemáticas mi idea es que esta frase es antes en beijing eso se esta frase le generamos un vector que resumen el lo que se está escribiendo lo que y luego vamos a tener entonces muchos padres texto que le corresponde y su descriptor de imagen y entonces vamos a entrenar una una vez neuronal para hacer regresión y regresión en el sentido de texto así imagen es decir un vector de texto vamos a entrenar que genere la imagen el escritor de la imagen correspondiente y eso lo vamos a hacer con las 20.000 imágenes y la idea es que logremos identifique que son descriptores identifiquen bien las keywords y cómo se ven ya sé lo que está haciendo aquí primero a todos los caps jones de los datos de entrenamiento generamos vectores aquí está haciendo el documento más y para no girar un vocabulario tan grande y justo en la zona que nos interesa que son palabras que no son ni tan frecuentes ni tan extrañas tiene que verse por lo menos dos imágenes acá y no más del 10 por ciento de nacer felicito mi palabra muy metida entonces no seguro que es más tarde escribiendo una imagen y bueno de esta forma nos quedamos con nueve mil palabras y hay algunos ejemplos y generamos entonces para los 1000 captions generamos 100 mil vectores de animación 19 mil 88 luego tomamos en los descriptores de de imagen que son de 2048 bloque extra large net a son 20.000 imágenes pero para poder ser los padres los vamos a expandir por 5 y ahí nos vamos a quedar con 100 mil vectores de imagen y por tanto tenemos 100 mil pares de entrenamiento y vamos a hacer la regresión de la dirección estadal aquí se demora y cómo lo vamos a hacer va a haber una red bien simple en una capa de entrada que es el que recibe el vector de la animación de input que son los de los de 9000 acciones esto es una capa oculta de 2048 y luego tiene la capa de salida que es la capa de él que genera el vector que nosotros queremos que genere el vector de la imagen que le corresponde al mentor de entrada de allí para ver cómo hacemos eso la clave es que pedir la activación transición lineal porque queremos que sea que tienen un vector no queríamos hacer pro centro bien y nace y él bueno y la otra clave es que la función del 2 es que sea aquí estaba calculando el arroz cuadra tipo medio y hay quien fue a tratar de calcular un vector lo más cercano en distancia utilizarla al vector objetivo de minimizar el error total de los vectores generados la distancia protector generado con su rector quien debió ver con su entorno real para el caché esto lo entrenamos esto una red esto somos menos grandes porque como son 9.000 de entradas de ahí hay varios millones de parámetros entre los 30 y los 256 pues tengo 2048 458 no hay tenemos más más millones de parámetros esto lo entendemos con 10 épocas y estos fueron que seamos no seamos un poco más de 10 minutos he entrenado con relieve ni sabes porque me lo uso para calcular el valor hecho y nos va viendo entonces que está verificado como bajó el 2 del training variación si se ve quizá podría bueno entender un poco más no se ve como invalidación todavía bajando y luego y una vez que entrenamos este modelo esta red neuronal está listo para el trabajo y quien lo que hacemos ahora es este modelo lo tenemos que guardar porque es nuestro modelo que relaciona el descriptor de texto con forma visual y cómo lo vamos a usar ahora ahora lo vamos a ver sobre un capaces totalmente nuevos distintos que lo que vamos a hacer ahora ya nosotros ya sabemos cómo ya tenemos una máquina [Música] una red entrenada que ha dado un vector de texto de animación' 9000 e pasa por una capa intermedia y luego genera un vector de 2048 descriptor visual donde está visual general entonces lo que vamos a ver ahora es distinto quien no tienen alguien con el que entrenamos y que el podemos pasar y tenemos un conjunto imágenes de un conjunto de imágenes que no las hemos edificado son imágenes sueltos ejemplo las las fotos que sacamos con el celular una etiqueta esas fotos de ahora estas fotos lo que sí tenemos que hacer es calcular de sus vectores visual por tanto aquí nosotros vamos a tener y luego dijo como matriz los vectores de la imagen uno de esto va a ser de 2048 y de la imagen una está la imagen en el teatro conjunto donde vamos queremos buscar nuestras fotos personales y estos son los pares tri que tiene que ser extraño con la misma 152 que con eso nunca entramos y que lo que podemos hacer ahora ahora podemos escribir cualquier palabra entonces decimos jirafas jirafa jugando para hacer esto generamos su vector de exide entonces girar y dinamismo vocabulario tras pensar él mismo define electorales aunque no tanto entrenamiento para que genera un vector de nación de 2.088 con las mismas propiedades que el club con lo que entrenamos y que este vector lo vamos a ingresar a nuestro radiación esto aquí va a generar un vector un vector de 2048 pero generado quién diría algo así como se tiene que ver lo que yo estoy escribiendo y luego vamos a buscar en el dato a ser el vector básica de la imagen la imagen que contenga él va a estar más cerca es decir si es necesario resulta ser desde aquí no en torno al cercano vamos a ver la imagen ya sin la imagen que te diga de decir jirafa jugando ya tiene un año entonces así es como vamos probar y de hecho aquí tenemos cuatro conjuntos de tests aquí está el conjunto de desde digamos algo distinto vamos a computar de ésta y vamos a buscar en según que las jirafas jugando y busquemos qué son aconsejables cebras de una cuidad y otros jugando [Música] vamos a buscar los dos vecinos más cercanos es lo que está haciendo aquí y va a buscar los dos vecinos es decir para cada imagen para cada una este texto venga un texto libre él esta es la qualy decimos generar su descriptor visual que es lo que estamos haciendo a este texto tenemos que calcular sus vectores usando el vector y zador del ff que usamos antes usamos el modelo que ya entrenamos y genera un vector de salida que no lo tenemos que normalizar porque estamos trabajando meter normalizado y ese vector es 'la ya que es como su vector visual generado vamos a calcular el vecino más cercano con todos los descriptores visuales y la imagen que está lo más cercana fue esta de entonces a nacer y la segunda más cercana fue de notar esto se debe a que estas son imágenes sueltas que no tienen caption y todo no lo estamos considerando y para cebras con jirafas y eso fue la primera acción fue por jirafas no considero en la celda y para niños en la plaza que aparecieron estas son sus fotos más países como quien está bien pero la plaza no busquemos entonces de huevos bien pero razonó y algo de eso plaza de juegos como que no cubrió el juego ni plaza ni puede ser dos razones una es que no toda tu entrenamiento no estaba y nunca socio bien en la relación la palabra plaza con un descriptor visual con él no hay plazas conste ahora si se ve quién nombra es una de las dos más cercanas y jugando béisbol allí aparecieron jugando sb que profesamos las palabras jugando y las dos máscaras son de mientras hay que ver si buscamos la zona [Música] 06 estaba fallando las cebras y béisbol jugando y no funciona entonces qué aquí se ve y dicho esto ya podemos ir un poco más allá para ver cómo está funcionando y usar que como estos estos de pacientes de entrenamiento y tenemos varios tenemos más datos [Música] de fútbol como estas imágenes estos datos se tienen etiquetas están etiquetados entonces podemos usar esas etiquetas y dice esta imagen 762 entonces damos la imagen 762 es ésta y ésta tiene descripciones tiene sus cinco descripciones en el anterior nos estábamos usando las descripciones pero las podemos al inquietas si usáramos ésta este texto como consulta deberíamos de encontrar esta imagen como en primer lugar si no ponemos esta esta descripción si esto está funcionando bien debería debería de encontrar que salió recordemos algo un poco más a girar tú mismo un número cualquiera 72 regreso y ponemos un jugador de tenis [Música] y ponemos esto como con susto y no salió más de cerca los vecinos más cercanos vamos a cambiar esto creo lima parece la descripción [Música] este texto aparece a veces que no serena williams de schering pero éste salió en posición 7 como acepte oficina más cercana quizás tienen algún significado si no están los datos de entrenamiento entonces les ha pagado no tienes que estar inmigrante se aprecia un séptimo lugar tan mal y entonces lo que vamos a hacer vamos a tomar los datos de entrenamiento sobre los captions de los datos de test vamos a buscar por esos captchas y vamos a ver en qué posición sale la imagen que nosotros sabemos que le corresponde el captcha y eso es lo que está aquí como son mil son 5000 casos son mil imágenes por 55 mil cartones por ejemplo aquí estaba probando de todo esto se demoró en nacer en evaluar en la gaceta en hacer 5.000 consultas cada uno de los caps jones ver sus vecinos más cercanos ver qué imágenes retorna y ver dónde en qué posición está en la que nosotros sabemos que él le corresponde escape y por ejemplo con la imagen 0 apareció en esas posiciones 485 para la imagen 100 10 03 está bien no aparece incluso nada siempre y hay 400 mil imágenes y por tanto se ve aquí que significamos todas estas posiciones tenemos 5.000 posiciones es rápido estos son cinco mil consultas cada consulta consiste en el texto sectorizando usar en la regulación y buscar decir más cercano de las de los mil vectores de 158 de ahí esto realiza 18 4 por segundo así que el buscador y radio en esta base de notar como de si todo fuera totalmente grasas y que no estuviera capturando nada una experiencia que en promedio quedarán en la posición 500 no se ve que en general a quienes visto grama que en realidad está funcionando bien y la mayoría está dentro de los primeros posiciones y aquí está el graficado por posición en tenemos perfecto un 5% de las consultas de las 5.000 que son 274 que es como el 5.5 por ciento retorno la imagen en el primer lugar no está tan bien represiva permitimos que tener primero en el segundo lugar es un 10% vea cómo mejora y si buscamos solamente en los primeros cinco posiciones ya son mis consultas de que esa línea de estos bloques para un estor hamaca 5 y por tanto 2000 mil 81 consultas de las 5.000 y obtuvo la imagen que estaba que le correspondía en primer lugar [Música] entonces captura la información no es lo mejor pero si funciona no solamente y no lea están todas dentro del ente de consulta a clientes así que claramente esto está encontrando relaciones entre palabras y características visuales de la imagen se ve que en posiciones donde igual falla y falla muy mal hay alguna que retorna la correcta en la posición casi mente y como para validar sé que esto en eso no necesita antes atento usando este se ve que son otras mil imágenes con captcha ni evaluaba mostrada casi lo mismo mil 65 imágenes y 1.076 en este sentido me parece los resultados bien entonces este este ejemplo se lo puede tratar de mejorar esto la clave equipo hilario que el dallas electric se lo jalaron más grande imposible y que lo la vectorización aquí estamos usando estamos usando te fines pero como es muy importante lograr discriminar las distintas formas de escribir una frase todas estas frases que se ven aquí todas estas frases para 5 frases son 5 formas de escribir lo mismo esencialmente hay algunos que vienen de la persona entonces algunas que resaltan cosas distintas en general todo se refiere a lo mismo pero no es que sean idénticas estas frases estas cinco frases porque algunos dicen cosas que el otro no no no resalta pero si hay alguna relación entonces el escritor yo mismo en aquí escritor ya está igual en todos el ordenador portátil y los dos dirá ordenador portátil computadora dice acá entonces esto empieza a funcionar mejor siempre dos arte fin a efe bustar amos algún descriptor el más semántico como lo pueden ver en algunas sus sabores ya sea por palabras o podría mostrar la web mover tistas o podríamos usar en universal sentencia de memoria algunas veces con alguno de esos esto crecería a mejorar cómo representar mejor el texto y las similitudes de los textos y con respecto a parte la imagen eso se podría mejorar calculando el global y tratar de hacer algo más por objetos objeto en contra y tocando usar algo más o algo para regiones