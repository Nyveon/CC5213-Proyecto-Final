Repaso ML - parte 2 - Algoritmos de clustering
https://www.youtube.com/watch?v=0VOqXdQwkY4
 ahora estamos haciendo un repaso de machine learning y aquí vamos a hablar sobre los algoritmos de eclass string del script son todos supervisados es decir se utiliza cuando tenemos datos pero no tenemos tipos de datos todos los datos para nosotros son iguales tenemos unas tareas más comunes tratar de descubrir qué grupos hay porque por ejemplo tenemos muchos datos de clientes y queremos saber qué tipo de clientes así que bueno entonces aquí el cliente sería la entidad del mundo real esto lo representamos por sus datos y poner aquí en una tarea previa es la de explorar los datos explorar los datos se refiere a algo que no hace desde mirar cada una de las coordenadas por ejemplo pero puede ser la cantidad de compras y luego podría ser no sé nada entonces no pueden mirar la edad de todos los clientes y la cantidad de compradores lo que busca el algoritmo de clustering es una buscar grupos brillantes es decir buscar entidades que se parezcan importante buscar vectores completos que se parezcan a los otros entonces de la totalidad de los vectores y cómo se hace eso la idea es que si como aquí es un de dos dimensiones podemos llevar el espacio de las dominaciones y cada uno de estos clientes va a ser un punto y aquí buscamos grupos de uno de estos datos y cuatro grupos son de hecho por ejemplo podrían el 4 de un albur y motivación porque 4 y eso podría estar bien pero también otros algoritmos podría decir que hay dos grandes puede ser o incluso que hay estos 92 habrá nueve tipos de clientes o cuatro o dos tipos de clientes porque cómo se evalúan y de hecho de depender de los datos en el contexto de documentados y como quiere tenemos un problema de negocio quiero mantener entidades del mundo real las valuaciones súper difícil y consiste en tratar de ver si que tiene sentido asociada a estos que asociadas a todos estos clientes a cada uno debe tratar de víveres o practicar círculo entiende cuál es el criterio de agrupación son los clientes que compran en la mañana y gastan muchas gracias y esto de aquí son el resto y si no lo eres contra el patrón en realidad no no no no tiene mucho sentido el distinto algoritmo de casting pero todos ellos intentan dividir los vectores en grupos y todos los datos asociada a algunos grupos y deben de cumplir los propios de los grupos uno es que los elementos asociados mismos grupos se tienen que aparecer así ya que los elementos asociados grupos distintos deben de ser debe de agrupar vectores que se aparecían y tiene que separar vectores que no se parecen y lo que haces puede tener distintas formas y tamaños y las ciudades y de hecho aquí en estas nubes de puntos será así que quieran cuánto clases habrá aquí y esto empieza aquí siempre salta la dificultad por la ambigüedad de la definición de qué es un placer entonces el primer algoritmo que vamos a venir más usado que se llama en el mismo de bikinis y donde que se refiere a acá también se refiere al pnud a clase se va a encontrar ya de 10 o menos tramposo 11 usualmente es parte de la pregunta a saber cuántos grupos hay pero aquí es así que uno mágicamente supiera cuánto clase una búsqueda y cómo funciona te dice como uno quiere encontrar acá clusters así generaremos cada semillas que van a ser como los primeros las semillas de los platters a las aves y luego vamos a eso para generar plaster y todos los vectores se van a asociar al azul plasta más cercano y luego con todos los que los datos junto a los datos perteneciente a un clúster calculamos del nuevo centro y estoy estoy aquí aquí hay un ejemplo obtenido de este libro este ejemplo está en estas islas que están ahí entonces qué es lo que pasa aquí en esta parte en la misma parte asociando estas fechas más a las ars es consciente punto este punto de este mundo esos tres puntos dividen en la base de tres segundos a su más cercano entonces todos los puntos de social son más cercanos y ahora todos los verdes que son los que te asociados digamos alguno uno por el centro de sevilla por esto una gorra y durante todos los verdes uno los promedios porque se supone que el centro del cluster triquis pero en esa instancia entonces uno borrar las semillas y ahora dice dónde está el centro de este plato y vas a ver si los azules y ese es y es la siguiente iteración y se ve cómo se mueven y efectivamente la siguiente iteración es que ahora como se volvieron ahora la separación y datos que eran verdes ahora pasará a ser rojos y entonces hay que de nuevo hay que recalcular los centros y se mueve por él se ve como una parte conocen al azar y luego los datos por sus densidades por sus dos pasiones by van moviendo el centro hacia los grupos de dos y luego esto por ejemplo aquí en este caso y tú converge aquí no convenio este dato y en verde a recalcular los centros se mueve un poco y esté de nuevo un poco y ahora esto pasa a ser del otro cluster de aquí entonces estos tienen que seguir liderando y en la creación 6 no ningún vector fue reasignado es decir la corrección fue pequeña y esto ya convergen entonces en el chasis convergen y este ejemplo las ostras activas entonces quiere ejemplo muestra como esto depende de la inicialización si los mismos datos ahora partimos con los centrales se mira aquí se da esta separación [Música] y luego él ve como el promedio de estos hasta el cobre y el promedio del congreso para estar más allá nos veamos cómo es la conexión y esa es una nave inicial y ahora esto estrella y ahora el centro del huracán sí se volvió un poco más la situación y ahí convergen converge el estrés son 5 convergen convergen a blacks es un problema de cádiz el que el resultado final sea aleatorio dependiendo del inicio es un problema que tiene varias propias una buena es que pero es entendible fácil fácil de implementar incluso de hecho uno puede programarlo no muy difícil es para él el que tiene alto paralelismo en especial cuando todos los datos se enriquecen a su centro y de eso no va a ser lento sino con multi corts de acelerar el cálculo y es relativamente eficiente en el sentido en el que si tengo o no hacer algún viandante y quiero calcular 10 clases cada uno de los 10 millones siente que sociedad es un 'cluster' más cercano y luego una integración de operaciones de internación entonces se van haciendo en literatura salas de internaciones hasta que converge entonces si a un millón por día un millón por diez una vez dos veces de presupuesto hasta que cumbre entonces eso es relativamente eficiente comparado con los otros resultados quieras aleatorio eso algo malo malo porque y sobre todo que termine un óptimo local como lo puede tratar de contrarrestar esto primero es que hay algoritmos de inicialización para tratar de evitar partido al azar conversiones lejanos entre sí o una forma mucho más interesante es calcular el algoritmo de forma incremental si no tienen los datos uno puede tomar un 1% son datos y sobre eso calcular el game ins diez ciudades más centrales del 1 por ejemplo los datos y luego uno toma un 10% de los datos y no tomarás son tres de anteriores como base para un siguiente gaming habrá sólo un día por centro de datos y luego son un 50 y sólo luchar y eso es mucho son mucho mejor porque sí sí que había ruido como no va haciendo selecciones al informe incremental sí que cayendo un óptimo local y luego cuando con una vez con más datos y al azar y usualmente sale desde ese último local de hecho la forma mental es muy útil para implementarla es muy difícil que implementar el sentido de que es lo que hay que hacer que hay que pasar y que el algoritmo que uno salva tiene este este método implementado un ejemplo de tal forma indirecta que hace un casting con un 1% y el y el output eso es el de inicio del siguiente del siguiente clase [Música] el resto que se ha incrementado solamente utilizarlo cuando el promedio está definido y decir eso porque hay un algoritmo y saber de medio x el centro y de es el promedio los datos el medio y de es la generalización la generalización de la mediana que es el dato que esta alza pero la gracia que tienen esos quienes no tienen que girar un dato al medio y hay veces condenación bueno por ejemplo los pasos médicos donde no tienen una distancia y no no hay vector por ejemplo los strings uno puede caracterizar las palabras una tira de distancia otra palabra de esta selección y no puede calcular el grupos pero no funciona con quilmes hay porque cuál es el centro en no strings y ahí es donde los medios pueden ser no tiene vectores en realidad de usar el algoritmo de medio de estos problemas y liz poco poca mejor vida y él como como todos los datos se elevan se asignan a sus centros de más cercano lo que va a estar bien iniciando es el error total como de redondear los datos se asocian al vector que esté más cercano con distancia y eso tiende a hacer que los clusters tienen datos del círculo de alrededor del centro y nosotros bocas 'cluster' circulares porque seas tú quien presta a sus crías y como no puede invitar cluster circular es bueno de una forma directa es cambiar la métrica la está su criada bola 3 por otros algo por ejemplo es muy comúnmente usado en alguna que sean las de más análisis quien lo que hace es calcular la distancia que está descorrelación al que es de hecho más cara no me ser lo mismo que hacer pese a de los datos van a decidir principales cinco tarde marciales sino que hacer la rotación que descarta la covarianza se hizo de calcular vista en su clear entonces lo que hace es quemar calcular del centro lides [Música] en un espacio de donde no hay corazones otra forma de mejorar estos plantes circulares es mediante la generalización que usamos en algoritmos expectation maxim station y el mismo algoritmo de cambien solo que ahora está basado en probabilidades ya pero la y es la misma y el ciclo de instalación es lo mismo son un kilo la norma son espacios que ya son probabilidades usualmente y modelado por medio de inox con cause jana y a veces lo que se llama una forma de gm en el dextro madre si los datos son grupos de emociones el mismo el grupo que sólo desde ahora con su versión project y se necesita así que es todo algo malo pero se puede mejorar el que necesites precipitar el número del 'cluster' riesgos efectivamente son algo malo porque uno no sabe no tiene que jugarse al valor normal y no equivocarse como no puede escoger el que depende de él de lo que no busque en el ciclo quiero que carlos más grande posible por otras veces en general hay un método bien conocidas al método alcohol que consiste en tomar el error total del plástico el error error cuadrática como el error de redondeo de todos los datos que es la suma de las distancias de cada dato al centro y del que fascina la suma de quadratín sumandos guardaron dependiendo el número 'cluster' sobre pronunciamos seguro que se reduce porque hay más claras de los dos temas siempre haber un cliente más cercano pero independiente como se reduzca el revés perros dulce empieza cuando ya cuando no hay clases son datos de forma que se están dividiendo las reducciones lineal pero cuando la reducción es mucho mayor aquí hay que escoger un método que la verdad después uno lo ve y es súper subjetivo el estado pero con datos reales por ejemplo despiertas y clientes que están comprando los tipos de clientes que nunca salen muy bien y uno no quiere y uno tiene 5 3 10 [Música] conveniencia de que todo esté funcionando es súper difícil de este problema de que aquí es que nos gusta ruido en el sentido de que sí parece clases bien claros pero siempre como un ruido uniforme de fondo este ruido uniforme entiende a uno de los clases para cualquier parte o agua o agruparlos clases para evitar que el mismo jerárquico en particular el algoritmo cooperativo es muy bueno para usar y se basan en lo que se llama esto en la matriz de distancias matriz de distancias ya que he aquí x bueno aquí está gráfico entonces el músculo está más cerca de la diagonal y la de canciones los cinco miembros de 1 n y de un año entonces esta matriz de imaginación a distancia de un valor de 60 y al mismo tiempo solamente la diadema la diagonal siempre va a ser puede mostrar que lo que uno quiere buscar grupos de mentores que están cercanos de entonces él le dice uno dijo que el grupo de vectores que están cerca de ventas y días normales entonces el mínimo global los dos elementos que estén más cerca de todos como justamente este que es el vector n 1 con el encontrarte con 36 y el segundo estoy aquí y estoy acá es sólo los dos gemelos más cerca y como solo que están más cerca tienen que estar en el mismo claro nos pasaríamos si los dejaban clase artística entonces lo que hacemos es todos no los sacamos y decimos porque es transformar el cluster el cluster uno la posición en el centro y estos dos datos se borran y la materia les diré en los dos datos y agregamos el centro y el centro de uno tenemos que agregarle y completar una final o la columna es una matriz de amor y esto tiene que tirar y tener porque ahora hay que tomar el mínimo global 10 y los 12 asociados continuaré único vector todos unidos este algoritmo es muy bueno porque permite ir viendo es difícil que se yo que el inconveniente primero es que calcular la matriz de distancia es usualmente prohibitivo viéndola ser de un tamaño normal neces ahora tiene un millón de elementos es imposible calcular la matriz 2 millón por un día la persona tiene el máster también elementos mil por mil un millón de datos para dar así es posible diez mil 10.000 datos de email por diez mil serían 100 millones algunos cientos de megas podría ser cien mil cien por cien mil el 100% del gasto ya se vuelve factible entonces usualmente en corta memoria ya es muy lento de modo límite al máximo de 100.000 datos pero el costo de operación para cada escuadra tico y 10 por cada por cada uno de los valores entonces esto puede ser hasta cubículo de hasta en implementaciones mejores y más rápidas por ejemplo estando los índice locales [Música] y eso cuadra tico para mí en operaciones es mucho y que sucede que se vuelve factible para ellos es la parte mala en la parte buena es que el resultado que uno tiene es teniendo grande que es la forma en que se van uniendo todos los datos este dato es muy bueno para realizar ahí no puede ir viendo 1 así como en cannes uno tiene que jugársela con el numeral cat aquí uno puede ir viendo todos los posibles campos quiero cortar con este umbral de distancia y tengo dos grupos pero podría cortar aquí y tengo cuatro logos el consejo 5 podría aguantar aquí por 0 a 35 1 2 3 4 5 6 nos cortarán tengo más luz a uno puedes decir y eso no significa que hay que recalcular nada solamente una porque además del programa como vimos cuando la mode funciona a distancia aquí va uno calculando distancia entre distancia y la jerarquía de los cámaras métricas [Música] este programa es muy muy bueno analizar es como un análisis completo es el mayor inconveniente es que para posicionar es del mundo grama no estoy hablando de combinados diez mil ya está con el límite de lo fácil tiene unos pocos miles de las dos o menos 200 datos de esto de lo mejor existe otra una tercera línea que sólo pasaban densidades del aire aquí es que uno y no solamente la distancia entre ellos y tienen dos parámetros un umbral de distancia y una cantidad de entonces todos los vectores que están a distancia menor o igual que una axila son parecidos y tienen que estar en el mismo cluster entonces si no ha tomado alimento de estos elementos están asociados y además se permite que sea es todo transitivo importando en realidad dos elementos y tener mejor que la tercia que existen datos uno más datos que están a estancias de también y por tanto esos datos acerca del lanzamiento de hacer que sean todos estos están todos asociados mismo claro porque existen datos en todos ellos están a distancias menor o igual a la práctica de y pueden aparecer a otro punto porque es el mismo y si es que hay porque si quiera y si hay un vector por acá lejos eso va a formar un 'cluster' único de uno solo segundo es el tamaño mínimo de elementos para el ser cocinado atrás sino el ruido que convertirán incompleto y de hecho el gran aporte de ventaja de este algoritmo y diferencia es quién aislado uniforme y datos uniformemente distribuido acá con que hay una línea unidos dea y eso le afecta mucho más a los uniformes de otra otra propiedad que tiene interesante es que puede encontrar hay clases de cualquier forma y qué es lo que tiene dos cosas malas además es que él el parámetro del umbral de decisión épsilon muy difícil y sobre todo aquí en dos de mano claro ver el resultado del resultado en el espacio de alta dimensionalidad y se vuelve imposible definir este umbral así que uno no tiene un objetivo sino que solamente el tractor exploratorio es muy difícil y pero aún en la estacionalidad como ya hemos hablado cuando la moda de dimensionalidad las distancias entre los elementos tienden a concentrarse en cierto punto y eso hace que el umbral de decisión sea súper difícil de encontrar porque está todo todos los elementos empiezan hasta la misma tasa entonces un exceso un poquito muy chico y no encuentra un poquito muy poquito más grande y eeuu y entonces eso hace que no sea muy factible de usar para el paso de dimensión y además para esto el mayor caso que sirve es para el caso de las dos para patrones visuales en imagen si no tiene analizar dado ejemplo estos son clientes y no quieren buscar clientes parecidos esto es que sea transitivo [Música] me sentiré que el primero con el último no se parece en nada porque está muy lejos hay un camino de parecidos entre medio pero una prioridad que todo se parece difícil y ahí eso no es el caso han encontrado dos clusters las tres no quiere decir que dos de las tres distintos como lo voy a comparar los de casting son los mismos datos cuál es mejor que otro como uno lo evalúa entonces aquí si tuviera datos real y yo sé cuáles son mis clientes tipos de clientes y quiero evaluar el casting que encuentran los tipos de clientes que yo sé que hay la verdad es que su plan muy ficticio porque si uno tiene de athletic y no sabe cuáles son los tipos de clientes entonces uno necesita 331 uno requiere un clasificador o algo supervisado es un momento difícil evaluar clustering con un gran club porque por definición de problema esto es no supervisado de ahí se usa cuando no hay dado el dinero y eso nos lleva a que comúnmente se evalúan de forma numérica y por medio de métrica matemáticas alejadas de lo que es el problema que están representando sino bien como por ejemplo error cuadra tipo medio o la dispersión o entropía cosa de sentido el coeficiente silva qué tan cercano hasta los datos con respecto a sus centros por ejemplo de intersección es eso a veces funcionan pero ya que quieren empezar estos datos como los queridos de clientes con con ventas y quiero verme tipos de clientes la única forma de evaluarlos ahí es tratar de hacer evaluación exploratoria y hay que visualizar todos los clientes de alguna forma y tratar de ejercer efectivamente sus datos si aparece o no y mejor aún lo que hay que intentar hacer es etiquetar la etiqueta en el clúster es decir uno tiene clientes y tiene clientes con su cantidad de compras y hora del día en que compra encuentra muchos muchos clientes que vienen a comprar zapatillas en la mañana a la mañana [Música] dice todos estos clientes ya parece y el problema de uno en el garage haití en el caso de versos esos 500 clientes que dicen que sobre el mismo cluster hay que mirarle descubrir cuál es el patrón y ponerle nombre al traste ya esa tarea súper difícil y es la que hay que lograr hacer es elevar y para hacer eso no se apoya en gráficos en gráficos y análisis durante cada clase no tiene que ir mirando los datos y tratar de interpretar cuál es la rutina que hizo que estos datos se agruparán aquí dice que se cumple y que sucede que que algo para los que están y se para y no se junten con nosotros bueno esta exploración visual es difícil bueno verte para finalizar este aparato está diseñado para el caso particular de la recuperación de información multimedia usualmente los escritores son escritores de contenidos de escritores de audios de escritores de medios descriptores de imágenes y estos son vectores de alta dimensionalidad solamente las bases donde imágenes no son bien imaginas son miles de imágenes de vectores de alta dimensión y eso hace que en general el algoritmo más usado de clustering que es el gaming no porque sea el mejor sino tiempo alguien de viaje el único posible de usar así que enrollable en acción de información multimedia todo es similar descubrir personas que hay en un vídeo o los que se usan como base intermedio para reducir un conjunto héctor y sólo con books ya que entonces bien cuando hagamos escritores locales la cura tiene un conjunto de vectores una nube de vectores pero calcula un quad box jesús sanz droides y luego todos los vectores se inscriben en este conjunto de centro es forman como una nueva un nuevo paso son como la base de normas y todos los vectores se describen en función de esos ser tres no no y ahí es un museo de centro inglés quien no tiene que ver con el precio que hay aquí uno usa muchos centros de aquí si tiene sentido los coeficientes en la evaluación numérica porque sientes y lo es esa cosa porque el centro tiene un paso intermedio del tumor de reducción de inversión