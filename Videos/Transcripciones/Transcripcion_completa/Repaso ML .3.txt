Repaso ML - parte 3 - Repaso algoritmos de clasificación
https://www.youtube.com/watch?v=y99uF3Pllso
 estamos haciendo un repaso de algoritmos de matilde y ahora vamos a hablar sobre distintos algoritmos de clasificación que un clasificador de segundos que tiene un conjunto de discretos de cada etiquetas o tipos de clases y cada clase tiene en vectores asociados el supuesto que los vectores asociados a una misma clase se parecen entre sí porque tienen en común que describir cuál es el patrón que agrupa a todos los vectores de una misma clase para poder rectificar datos futuros nuevos datos y encontrar más datos que pertenecen a esas clases verdad todo en lo que vamos a hacer es que hay que descubrir más funciones efe que va a dar y arreglar si quiero vectores sondear y dimensiones de red hace que es el etiquetado de aquí dado un vector contenido cuál es su clase y esto df esta función es un núcleo el clasificador analiza el espacio de las generaciones y tiene que encontrar regiones y encontrar las fronteras y luego hacer esa función de entrenarla vamos a contrato con un ejemplo teníamos descriptores y sus etiquetas entonces aquí tenemos un aunque aquí tenemos clientes y tenemos los clientes tipo uno los clientes tipo todos los clientes tiempo 3 y esos cliente esté representado por sus vectores y esto necesito estos vectores son los clientes tipos que serían los clientes que compran mucho los buenos clientes y no estaba representado por estos y los clientes que compran poco y estos son sus datos de cómo una ingreso su puesto base es que todos los clientes del mismo tipo esos pintores que hay que parecer y por tanto estamos capturando las las coordenadas correctas si no estamos calculando las corderas correcta esto funciona básicamente al nacionales entre sí y de hecho es visualmente este sistema como que todos los vectores tipo 1 tienen que estar cerca 3 y los vectores tipo 2 distancia que antes y los tipos cercanías y el algoritmo de clasificación lo que hace es encontrar la región que agrupa a 2 sobre 3 es decir estas fronteras las fronteras de regiones es lo que está buscando un clasificador el algoritmo de entrenamiento y una vez que esto está entrenado los datos de entrenamiento se pueden borrar y que el moderno y es templadito el modelo está en línea lo que esto es el modelo qué dice el espacio de meditación es como donde están las ganas las regiones que pertenecen a cada tipo de planta buscan y ahora sí queremos cuando eso está entrenador o cuando vienen datos nuevos 17 es el cliente del mundo real x calcular sus características de su representación y estas representaciones que pasas tras la función para que diga qué clase es internamente lo que hace es que esto esté este vector se lleva al espacio de diversiones de la región donde cae y retorna la etiqueta que le corresponde entonces este vector de aquí este es un tipo 2 por qué está claramente excepto el nosotros estamos con un esfuerzo de 95% de los datos del clasificador entrega no sólo dice la clase la quiénes sino que se entrega un cierto valor de confianza de su respuesta eso sumamos que tenemos los clientes con sus datos y que en este zona g es un tipo 3 del argentino 3 pero está cerca de la otra región un clasificador un buen algoritmo clasificación dice es con tipo 3 pero bajaría su score porque está cerca de otra clase y podría ser que se esté equivocado importante elevar el escorial y ese score de clasificación es clave después para usar el usar el clasificador y ajustarlo con datos para verlo producción de unos 11 ponen a veces cuando un clasificador de simití me bajo una imagen hacia atrás juega por una prueba una pluralidad baja dependiendo de los riesgos o del error a veces uno prefieren que el sistema no diga nada yo casi posiblemente serán empleados por alguna esos son cómo funcionan los clasificadores notar lo que lo más clave de esto es que todos los algoritmos de clasificación hace lo mismo usualmente los resultados que logran cierto clasificador y nombrar con con otra clasificación debieran de obtener los mismos datos entre los mismos resultados lo que porque genera iniciar cuando un clasificador les va mal y sacándose un 30% respuestas correctas y valor en contra del patrón otra misma clasificación tampoco va a encontrar la respuesta ahora es decir a un 30 por ciento otros clasificadores con distinto alcoholismo pueden sacar un poco más un poco menos provincia 30 no nos va a saltar a un 90 por ciento con otro clasificador si sucede uno diga que el clasificador original una haciendo algo más y en el mar bueno esto para el caso para el caso general es distinto algoritmos de clasificación sólo distinto a lo que haríamos tri que como definen esta región y buscan los grupos y tienen ventajas desventajas de las cosas distintas [Música] intento los distintos ritmos de clasificación lo más típico esta logística que aún cuando su nombre está refinación en un clasificador el test miliciano en el árbol de decisión el clasificador karen no confundirlo con las búsquedas cadenas el clasificador que es un clasificador que se basa para clasificarse se buscan de la manera vector del soporte y su perfecta arma chin era lo que antes de que los mejores y las reinas [Música] fallecieron se basan en una teoría en realidad evidente de valles historia valles dice que sí dado un vector la probabilidad de que sea así tengo tengo este dato que tengo este cliente con estos datos la probabilidad de que sea en esta clase es un buen [Música] con el bayern se puede convertir en calcular la probabilidad de que para un cliente bueno un cliente no tenga estos vectores tenga estos datos porque porque esta probabilidad de que se puede calcular con datos por supuesto el vector que se está evaluando se puede calcular ahora bien y rivas t usualmente se descarta cualquier porque es la misma para todas las clases y uno quiere saber cuál es la clase más probable y tanto es dividiendo sobre este valor así que uno termina generando se vale entonces como se entrena esto supongamos que tenemos vectores de dos naciones que no sé la edad y más del ingreso entonces tengo los clientes tipo 1 y los de tipo 10 naranjos uno empieza a ver todos los coordenadas la coordenada uno de los malos también tenemos esto de una forma independiente se separan uniendo todos los datos es necesario un muy buen clasificar de distintos como es el uso de esto sumó que aunque viene alguien y dice tengo un dato y dice tengo en la coordenada 1 152 y la coordenada tengo 350.000 el ingreso y la altura 152 en efectivos entonces como no usa esto tengo algo de lo entrenado lo que hay que hacer un salto de lema vallés la probabilidad de que este vector de que el elector sea tipo 1 o tipo 2 y cómo se hace eso tan teorema valles la realidad de ese tipo no es lo mismo aquí la probabilidad de que el tipo 1 genera este vector y como uno hace eso uno tiene que ir a ver aquí el tipo de clases al cual la probabilidad de que esta es la clase 1 haya generado este reto en la parte como estamos definiendo las víctimas si lo hacemos como yo lo más de 1 a 0 como pluralidad en forma independiente entonces calcular y bueno va a salir de ahí 12 así como propiedades para comentar que tienen su caso muy bueno que es [Música] es natural eso también cuando ya está esta base es probabilístico si no puede hacer removimos bien elaborados y si no empieza a tender eléctrica está modelando uno puede poner ese convencimiento previo en las distribuciones de probabilidad y no parecer una similar es muy bueno el árbol decisión es otro enfoque y consiste en que uno tiene que tener todos los vectores de los tipos de los tipo dos y uno va viendo por coordenadas en forma independiente la coordenada que mejor separa el valor que mejor se para los tiburones tipo 2 en esta orden y entonces un agua si hubiese un poco de algoritmo de escala de tripp [Música] la idea que aquí uno tiene la asignación o asignaciones y mezclar los tipos últimos dos viendo por cada asignación cuál es el valor que lo gracia para el mejor equipo del mundo último dos y uno escoge el algoritmo que es mejor para los tipos por ejemplo este caso bueno tanto entrenamiento dijo que la coordinada del ingreso estimaciones de ingreso y estados y dinero entonces lo que mejor te es que yo les tipo nos tiramos los tiempo uno es una persona que compra un videojuego y el último 2 no comprendió entonces según estos datos aquí dice que el mejor separador era el ingreso y 900 mil 900 mil ingresos y para un lado y aquí se separan muy bien y luego hay que en este caso hay que preguntar por la edad y si tiene menos de 30 euros activos es tipo 1 con una certeza a 80 por ciento o si lo que vimos es un tipo 2 con una sentencia de esto el almacenamiento lo que hizo fue ir viendo la matriz de datos e ir creando estos estas separaciones y cómo se evalúa 80 bajar por el árbol y ver dónde está y aquí sale a ver todo esto con mucha pausa usualmente estos árboles de decisión el mayor inconveniente de la decisión es que este árbol no puede bajar hasta el final hasta hasta que se decida según el dato entrenamiento qué clase es usualmente se obtienen malos resultados porque tiende a ser utilizado en los vértices pero prefiere cortar un poco más de los árboles la precisión tiende a los demás y además porque tiene un único árbol de una mejora para esto es que en calcular varios al mismo tiempo y recorrido al mismo tiempo es una especie de votación de los distintos árboles se van dando fuerza quiero un clasificador muy bueno y rápido algo y algo bueno del árbol es decisiones binner si no está haciendo data saints este árbol es muy bueno para entender el comportamiento si no tiene entender el espíritu con las personas que compran y otros este árbol no está dando información de negocio súper importante ya que nos dice programas de publicidad preocupa que esto no sucede el can n lo que hace es que usa las búsquedas carne m para clasificar y el concepto aquí es simplemente uno dice quién tengo datos de entrenamiento con sus clases tengo un dato nuevo que desconozco y tengo mis ratos con sus clases de la etapa de entrenamiento es nada maíz my es clara bien tocar o podría ser crear un índice europeo porque también es un dato que quiero clasificar y lo que no hace quien hace una búsqueda knn en la amarilla no sé qué clases a una búsqueda en este caso de unos 5 más cercanos y sus cinco más cercana salieron de esta forma el primero era tipo 2 el último no oyen interactivos y hay un tipo un tipo último de estos ahora cuál es la decisión aquí es que uno es una votación y saliente un tipo dos que en este caso 60% no votación bien simple y que dice ahí planta 5 eso es una forma pero al menos es bien intuitivo cómo funciona dice busca los clientes dime qué clase de esta persona desconocida lo que busco dentro del cliente que conozco a su más parecido y la clase es más parecido en la clase y para hacer un poco más robusto uno busca el primero busca los cinco tiene una propiedad de votación ahí hay distintas formas de hacer vacaciones por ejemplo aquí en votar votar 543 washburn entonces uno va sumando el primer lugar como estoy buscando los cincos y lo más cercano del primer lugar votar 5 tiene un voto por 5 cosa vale más importancia a alguien sino más cercano y tienen 4 entonces algún voto de sestiba bonos de hacer votar que él pierda 1 el segundo cuarto desde otra forma de preocupación en la oportunidad de este clasificador es que las propiedades de la propiedad más bien es quién no no entrenan sino que guardan los datos de memorizar datos está memorizada los datos de entrenamiento y después le gusta mirar durante la clasificación está usando tratos de datos se dice que el xxi en su aprendizaje porque lentamente el aneto es muy rabia una propia muy buena de esto es que con un solo dato esto ya empieza a clasificar el sistema tiene muchos una nube de datos y allí hay un tipo 4 que solamente tengo un ejemplo en el caso super es súper escaso uno tiene ese dato y ese dato ahí hay una región de donde ese dato es el vecino más cercano entonces puede que puede encontrar un dato nuevo y eso es algo que es buena ley si es que los datos de entrenamiento pueden ser dinámicos y digno podría por ejemplo tener un clasificador según las compras 200 sé que los clientes que han ido comprando que son buenos [Música] y los datos son dinámicos y las compras tengo más datos de clientes y los clientes desconocidos que quieren estar usa los datos de ese momento [Música] es algo que no tiene que entrenar un recibidor bayesiano un ático para el mundo entrenar y después empiezo a usar a predecir segundo periodo a veces es el único clasificado posible así que uno no puede darse el lujo de tener el mundo para estrenar algo que viene y una propiedad muy buena es que uno puede dar una justificación de por qué respondió algo ya lo que se llama la explica habilidad es decir si hoy pero por qué por qué me dijiste y yo era un tipo o un plan cliente bueno es muy fácil decirlo porque al tener un cliente bueno porque te pareces a este otro cliente que sea bueno pero el siguiente está malo es que aquí cuando empiezas a ver evaluaciones como no puedan estar justificación y pasado entonces no está el sistema producción de uno le reclama el dato está mal etiqueta reclama que el clasificador dio un error uno puede justificar cualquier dios ese error y usualmente era un dato mal etiqueta y eso nos permite entender qué es lo que está más amplio cuando un sistema del mundo real en la industria en poder explicar lo que está pasando y algo más que responden y responden solamente si se equivocó uno no puede decir porque señor solo que si dios permite un entrenamiento que responderse super machine es un clasificador lineal que separa los datos en primer plano en este caso [Música] lo que hace aquí es que tengo esto son las clases 1 estos son las clases 2 y hay que calcular el híper plano de separación el plano dice para nosotros ya y entre estos dos y la línea me une la línea 2 las dos posibles son igual de correctas en el sitio de las 12 para las dos clases pero aquí se ve que está muy lejos a esta línea 2 porque aquí está hay un margen muy pequeño y uno debiera preferir las líneas que tratan de round no pasar muy cerca de un dado porque después los datos futuros se supone que van a estar como por aquí entonces como van a estar un poquito más allá de que la frontera esté lo más alejada de los datos posibles su perfecto y así encontrar el plano de separación que se llama maximiza el mágico está la justificación de que es el margen de uno y esa imagen del otro y uno prefiere llevar al mar y ahora el tema es cómo encontrar ese plano que maximiza imagen normal que se define y todos los vectores uno puede calcular el origen entonces este rector es así el producto punto la proyección desarrollo entonces va a decir si está o no qué tan lejos está del inventario oye dice que está a un lado o del otro bien entonces lo que hace es que mediante esta formulación no puedes saber si este es el vector esta aula de lengua el plano o del otro 10 por tanto porque es binario y lo que hay que hacer es encontrar el mejor híper plan de separación y él la condición clave que se para aquí porque es tan bueno superlento machine es que no hace una decisión global de todo el a base de notas de maximizar el margen con respecto a todos los negros de casi todos los puntos blancos de acá sino que solamente interesa maximizar el margen con respecto a lo que están en el borde es decir este punto es este estos tres definen el sleeper plano de separación y sólo esos tres da los mismos que hayan 100.000 datos del espacio nasa acá wild ataque a incitar lejos del primer plano no definan la imagen no influyen en el ejercicio de magia y eso que están y esos datos quienes tienen la decisión y la forma o la ubicación de la imagen del impacto de separación esos datos se llaman los vectores o lo que soportan la decisión que está tomando de cortar el concepto blanco y cómo se encuentran esos de hecho para encontrar el primer plano es un algoritmo matemático de la optimización de verde hay que bueno hay que maximizar el margen o boné y hay que minimizar esta fórmula surgiendo a esto aquí estoy aquí en la clase si es un número uno y estoy aquí es la distancia de maximalistas walter o para que sea mínima entonces él es un programa de optimización porque el algoritmo de optimización tiene algunos como los matemáticos aquí volviendo al ejemplo acá qué pasaría si es que hay un dato negro justo porque está mal etiquetado digamos eso sería problemas de optimización es fatal porque cualquier vivían quien asiente en la solución no contenía una solución muy mal una solución que era muy buena se arruina porque en este dato no no no cumple uno no cumple el sujeto como para ser un poco más suaves el sujeto al y donde aparece lo que se llama el margen suave y que el sujeto a nosotros estamos permitiendo un pequeño error y esta constante cede todos los super mente machine es el costo del que estamos dispuesto a pagar con un dato mal etiquetado del rombo por un dato mal mal mal asignado imagínese tiene la correcta y usualmente esto hay un costo un costo de descubrir un parámetro tiene difícil depende de las clases [Música] el kernel lo que hemos dicho hasta ahora es que es un híper plano separación por tanto sería un clasificador lineal si las líneas y si los datos no son línea para que no funcione suplemento machine que utilizaba el que invertir y que lo que hace es que los datos si bien no son linealmente separables los vectores originales no nos llevará a un espacio de mayor dimensión ahí sí serían linealmente semanales y lo que no tiene que esperar es cuál es el carné él se extenderá a la función que uno va a usar para generar más financial es para llevarlos en espacios de mayor dimensión por ejemplo si tenemos los naranjos en la clase 1 los azules o la clase 2 no hay recta que logre separar bien estas dos clases entonces no posible quien se parece en estas clases lo que hay que hacer una demostración los datos en 2d se llevan al 3 de esa tercera dimensión fue generada artificialmente mediante este kernel un que en el polinomio y lo que hace es que en ese usando esa tercera dimensión artificial en el espacio tres de las dos clases y son linealmente separables y ahí uno puede calcular el híper plano en el híper pleno de separación de que quien logra separar la clase y ahora espera más y es que se volviera al espacio original uno de día que el psm que se obraba por plasma en el espacio original en realidad está separando por formas por otras formas y por un círculo en el espacio en el espacio original y de hecho con con un kernel y suficiente tiempo puede generar formas arbitrales y hubiese como cualquier forma arbitraria para clasificar el binario y se generaliza a múltiples haciendo el usual en lo más común acción de los inversores cuál es el score de confianza de un así finalmente el súper welter machine el modelo entrenado es un par de lectores que son los que hicieron de su muerte ya que sólo tienen el libertad entonces cuando viene un dato que dice no tiene stern y datos no va a limitar él es fuerte confianza de riesgo la distancia de planes semanas y dice que está muy cerca puede decir los dientes clase 1 pero no podría equivocar porque está muy cerca del corazón excepto cuando hay múltiples cuando en políticas como son fueron one person y lo más común si dice que es clase 1 con un 20 por ciento clase 2 con 70% y clase 3 con 60% ok estamos claros que de su perfecta el machine tiene que decir que es clase 2 porque el que el que es más probable ahora pueden la probabilidad hoy la confianza que es emplazados en esos países y ahí hay ahí depende del software como calcula esta probabilidad y por último están las redes neuronales de aquí en lo que hace es que da un vector de entrada hace estas operaciones y genera un vector de salida es muy interesante ver que es un vector en este caso de cinco dimensiones que se va operando hacer nuestras operaciones de multiplicación y sumar con dos pesos y esto de aquí este vector que tenía ciertos normales y entonarán va generando otros vectores intermedios y en este caso es la misma idea de lo que vimos recién con su perfecta machine si uno lleva a los vectores a un espacio de más alta dilación que es más fácil son más fáciles de separar esencialmente eso lo que está haciendo también está llevando las de entre medio estaba como creando un espacio donde sea más fáciles de separar los datos y cuando es tan fácil de separar lo puede reducir a una dimensión baja y esa reducción baja es el vector de salida aquí es en la clase en la que uno tiene que tratar entonces uno trata del vector de el vector característico y su salida es una clase la clase en la base de todo esto el perceptor que cada uno de estos circuitos está ahora y dado un dato de entrada hacen unas operaciones hacemos estos son sus pesos ya operan multiplican los pesos por la entrada mediante esta fórmula no podemos simple una forma de alinear por aquí más de 15 días más y que se puede entender como que aquí tiene un valor un peso un peso libre [Música] segunda lección submarina quien no está asociado a nada y quien permite que lo que hace esto esto genera una función de salida tiene una idea pero hay una función de activación antes quizás que se llama de activación porque la idea de aquí es que estos percepciones se enfoquen en buscar cierto patrón en el en el vector de entrada y si es que lo encuentran quieren propagar un lo encontré o no lo encontré y entonces distintos perceptores se van a ir enfocando en distintos patrones que busquen en los datos centrales de la función de activación la idea es que hagan una decisión de si es que esto de aquí el valor que encontré aquí es suficientemente bueno para decidir que lo encontró el padrón en esta foto que activa su mano no tantos detalles pero es la función de activación entonces es una función que nos permite decidir si es que la última un dato real entonces supongamos que esté sujeto a la entrada 240 en algunos y estos son sus pesos y aunque existen y para esos pesos su salida fue dos puntos operando y sumando el total pero es que este dato la salida esperada para estos pesos y para este dato cometió un error de que salgado 2,3 menos 4 tiene otros datos con los mismos pesos que la misma red y ahora con otros datos tienen que tener estos datos y usaría fue 1.4 pero esperado para eso de las 3 entonces cómo comete más errores finalmente el error total consiste en pasar todos los otros entrenamientos y ver el ebro el esperado y una suma y es el valor total y ese es el error cometido por estos versos y las ideas que uno puede intentar reducir el error ajustando un poco atrás y entonces qué ve que lo que recién he man presentarte para cada uno de ellos cada uno tiene un valor target y el error cometido entonces ahora tengo un vector de entrada 10 no sea el cliente isabel market un cliente que es tiempo 3 de paso por la red con los pesos como tenía y fue entonces está cometiendo un error y entonces uno va hay que sumar todos estos errores y después llegar al mínimo de propagación de ideas que aquí dice tengo tengo un 1 y esto es lo que necesitaba generar un mundo el cuánto es la fracción que tenía que no esperaba a quien retornar este valor de aquí y esta justa y su ajuste lo propaga hacia los aceros anteriores y todos se van ajustando y cómo tienen que ajustarse a múltiples entradas tienen que ajustarse a todo un poco y sumar un sumar todos los errores el algoritmo de más provecho es la idea es que intenta corregir los pesos para ajustarse a todos los datos entrenamiento animal y minimizar el error total en los que el conjunto de todos los pesos [Música] y se van actualizando los pesos se llamará chrome una parte desde el final y ahí nos va propagando los diferenciales para extraer ajustando los pesos y aquí sale el algoritmo ex amante de la distracción de la idea es que cierto conjunto de pesos alquila una función de una red neural puede tener 100 mil pesos ajustado y por tanto es esto de aquí si el espacio de alineación 100 mil y para el conjunto de 100 mil pesos de la red de en ese momento generó cierto los ciento de acierto o error total y la idea es que una justa lm entre los parámetros para que reduzca en su grupo vino ajustó un poco los que si uno paga muy bien moviendo con ésta por el error estaba tratando de minimizar el error bueno y por qué se llama de gradiente ascendente porque uno trata de escoger el ajuste que vayan máximo desde el máximo descenso es en el plan el menor game de una reunión más tiene parámetros que son los pesos y los pesos se ajustan automáticamente mediante esos parámetros y se ajustan tratando de minimizar los que el error total que está cometiendo para entrenar para clasificar los parámetros ese parámetro quien define cómo se ajusta a los parámetros terminal el entrenamiento es el entrenamiento consiste en ajustar sus palabras de los internos y el entrenamiento por ejemplo al caso a la red de la arquitectura a la red cuántas capas de activación [Música] [Música] está muy bien explicado 32 didáctico y visual de cómo funciona el algoritmo de facto proyección más allá de las que no es así learning se refiere al uso de redes neuronales muy grandes diferencias con una red tradicional la diferencia es que primero existe y que requiere grandes datos para poder entrenar ese tamaño pero la entrada la entrada es que se entrenan para analizar de datos que no son vectores cuando son vectores de una red de mlm de cola que la tiene acabamos de ver dónde tiene un vector de entrada muchas capas y un vector de sanidad este este caso es puede ser resuelto mediante todos los otros clasificadores que hemos visto y directo y la red neuronal no tiene ninguna ventaja como inconvenientes deja para este problema y por eso era que en la red neuronal ya existe hace mucho tiempo y no eran muy bien vistas por qué no no presentaban ventajas con respecto al clásico que deseaba y en particular con su perfecto machine super vector mantiene lejos mejor para encontrar un mejor mejor plan en su variación cuando se vuelven importantes las redes neuronales la mlb en sí misma no tiene gran ventaja pero las redes que se pueden usar para analizar datos multimedia ese fue el primer el primer gol que es lograr y lograr las redes neuronales y que viene dado por otro tipo de redes la red convencional y porque son importantes porque permiten calcular la representación analizar el dato y generar su vector característica y es la característica clave que define a de blanning y la que hizo atendible sea lo que obtenga el resultado aquí en entrenar bien pero que aquí vemos 90 según este problema de vector asociarlo al vector de hacer y entonces hay que tener un poco más clasificación imágenes si tiene tres imágenes tres tipos de imaginas y uno quiere definir te pasa y decir quiero que la salida para cada uno ciudad en 2010 en la red mlb que vimos recién desde el limbo es un vector y si el importe es un vector y tendrá una imagen lo que como como no clase como uno le clasifica en esa imagen hay que convertir la y es que esta imagen línea avisarla a un tamaño muy grande debe ser no sé cuántos píxel ante una imagen 300 mil píxeles es una imagen normal d esta imagen píxel a píxel y aún así eso no funcionaría funcionaría por stella pero no generalizar lo otro es tratar de generar un vector uno a mano el argumento que es de hecho lo que hemos visto en el cursor en eso generar vectores por ejemplo un programa de colores un programa de bordes y lograba por zonas llevamos un vector y es el vector que ingresa acá de hecho esa es la técnica que se usaba hasta y él así que lo difícil es cómo lograr encontrar los protectores porque porque uno veces son bordes más colores y ahí se supone que instalase balanza pero el caso la característica clave visual que permite discriminar y diferenciar a un gato negro hombre como para poder no sólo quedar un vector que represente la cual en la que critica clave la respuesta mack son los colores no serán formas redondas algo con las formas que adquiere la forma parecida por eso parecía pero para nosotros es evidente la diferencia es la dificultad que aborda en diferente y el machi learning que dice no tratar de codificar y reglas sino quien pasa de datos pero eso proba sobre datos pero aquí se pasan electores porque pasa nada pero lo clave es que pasan imágenes la imagen y yo analice una imagen y eso es una reconversión al la red convolución al es una red que recibe una matriz una foto una matriz y qué calcula mediante convolución es el filtro convolución es un filtro que recorre la imagen buscando haciendo algún proceso sobre la imagen y la gracia aquí es que el filtro los números los valores del filtro los vamos a entrenar y por tanto cuando estás haciendo ese movimiento está como escaneando buscando en la imagen y como esto es parte del entrenamiento de aquí viene la clave de cualquier disfuncional emelec porque se pueden enganchar redes convencionales confirme el epre todas juntas en una gran red un unificado y tiene un clasificador al final un clasificador con salida de 3 a 4 perros donde tiene vectores pero tiene una etapa inicial de análisis de la imagen y que por tanto entren a los patrones visuales que deben buscar la imagen para poder discriminar cuál es la diferencia truncando lugar y eso y eso a mano como se puede definir pero si hay algún patrón del tema es cómo escribir lo viví por eso en el mar chileno porque está esto fue que está y no sólo eso un patrón busca un patrón visual por ejemplo podría tratar de buscar en triángulos si si los presos del patrón fueran cosas y no por uno especialmente como que estaría buscando diríamos por ejemplo tenía buscando orejas y eso permitiría discriminar si el perro de eso pero no me siento que éste está cargado está buscando ovejas este defecto la cara va a estar buscando hojas este filtro acá va a estar buscando a la espina no ganas y cada filtro genera una imagen con el filtro aplicado y por tanto se va a resaltar la aparición de dónde está la oreja dónde están los ojos donde hay alas de avión y las traseras y luego otra convolución que opera sobre el resultado de las conclusiones anteriores puede empezar a ver las relaciones entre gatos se puede saber está buscando operaciones dos orejas jotas de dos conjuntos y ahí es donde empiezan a aparecer los patrones complejos entonces al anidar las conducciones uno empieza a buscar la inclusión un filtro muy simple visual pero en la unidad revolución es simple si uno empieza a hacer a buscar patrones complejos la hizo también es es algo que es bueno de la red convencional si no buscar patrones más complejos pero son todos simples y eso es lo que hace que la red conclusión al desistieran del super resultado y que esto él en el estandarte de lo que vamos a hacer es que ahora en el curso vamos a ver cómo se usan más que más que como entrenadores sumando un curso más grande aquí va a ver cómo usarla para hacer recuperación de información